{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom datasets import load_dataset\nimport torch\nimport json\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:00:28.368660Z","iopub.execute_input":"2023-11-28T15:00:28.369101Z","iopub.status.idle":"2023-11-28T15:00:48.471342Z","shell.execute_reply.started":"2023-11-28T15:00:28.369058Z","shell.execute_reply":"2023-11-28T15:00:48.470510Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"mistral_checkpoint = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\nmistral_tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint)\nmistral_model = AutoModelForCausalLM.from_pretrained(\n        mistral_checkpoint,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\nmistral_device = next(mistral_model.parameters()).device","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:00:48.472976Z","iopub.execute_input":"2023-11-28T15:00:48.473645Z","iopub.status.idle":"2023-11-28T15:03:55.659005Z","shell.execute_reply.started":"2023-11-28T15:00:48.473613Z","shell.execute_reply":"2023-11-28T15:03:55.657884Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a42f48975ef462ea4d7f63186e9d420"}},"metadata":{}}]},{"cell_type":"code","source":"t5_checkpoint = \"t5-base\"\nt5_tokenizer = AutoTokenizer.from_pretrained(t5_checkpoint)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t5_translator = pipeline(\"translation_en_to_de\"\n                         , model = t5_checkpoint\n                         , clean_up_tokenization_spaces = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_gen_text(prompt, model, tokenizer, device):\n    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n    \n    generated_ids = model.generate(**model_inputs\n                                   , max_length=1000\n                                   , pad_token_id = tokenizer.eos_token_id\n                                   , do_sample=True)\n    gen_text = tokenizer.batch_decode(generated_ids)[0]\n    gen_list = re.split(r'[.!?]', gen_text.replace(prompt,\"\").replace(\"<s>\",\"\").replace(\"\\n\",\"\"))[:-1]\n    \n    return [x.strip() for x in gen_list]","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:04:03.566996Z","iopub.execute_input":"2023-11-28T15:04:03.567294Z","iopub.status.idle":"2023-11-28T15:04:03.574128Z","shell.execute_reply.started":"2023-11-28T15:04:03.567267Z","shell.execute_reply":"2023-11-28T15:04:03.573136Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"output_path = \"output.jsonl\"\nbatch_size = 50\nnum_iters = 100\nlast_index = 0\ndata_list = []\n\nfor i in range(num_iters):\n    prompt = f'''\n                Write {batch_size} different short sentences.\n                '''\n    gen_text = get_gen_text(prompt\n                             , mistral_model\n                             , mistral_tokenizer\n                             , mistral_device)\n\n    translation = [t5_translator(f\"translate English to German: {x}\")[0]['translation_text'] for x in gen_text]\n\n    with open(output_path, \"a\") as f:\n        for j in range(len(gen_text)):\n            tmp = {'id': (j+last_index), 'translation': {'en': gen_text[j], 'de': translation[j]}}\n            f.write(json.dumps(tmp) + \"\\n\")\n    \n    last_index += len(gen_text)\n    print(f\"iteration: {i+1}/{num_iters} | completed: {last_index}\")\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the path to your JSON Lines file\nfile_path = \"/kaggle/working/output.jsonl\"\n\n# Define the dataset configuration\ndataset_config = \"json\"\n\n# Load the dataset\nmy_dataset = load_dataset(dataset_config, data_files=file_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}