{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U datasets huggingface-hub","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-30T20:48:38.581799Z","iopub.execute_input":"2023-11-30T20:48:38.582479Z","iopub.status.idle":"2023-11-30T20:48:49.993420Z","shell.execute_reply.started":"2023-11-30T20:48:38.582436Z","shell.execute_reply":"2023-11-30T20:48:49.992638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom datasets import load_dataset\nfrom huggingface_hub import notebook_login\nimport torch\nimport json\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-11-30T20:48:59.392658Z","iopub.execute_input":"2023-11-30T20:48:59.393148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mistral_checkpoint = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\nmistral_tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint)\nmistral_model = AutoModelForCausalLM.from_pretrained(\n        mistral_checkpoint,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\nmistral_device = next(mistral_model.parameters()).device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t5_checkpoint = \"t5-base\"\nt5_tokenizer = AutoTokenizer.from_pretrained(t5_checkpoint\n                                             , model_max_length = 1024)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t5_translator = pipeline(\"translation_en_to_de\"\n                         , model = t5_checkpoint\n                         , clean_up_tokenization_spaces = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_gen_text(prompt, model, tokenizer, device):\n    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n    \n    generated_ids = model.generate(**model_inputs\n                                   , max_length = 1024\n                                   , pad_token_id = tokenizer.eos_token_id\n                                   , do_sample=True)\n    gen_text = tokenizer.batch_decode(generated_ids)[0]\n    gen_list = re.split(r'[.!?]', gen_text.replace(prompt,\"\").replace(\"<s>\",\"\").replace(\"\\n\",\"\"))[:-1]\n    \n    return [x.strip() for x in gen_list if len(x.strip())>3]\n\n\ndef push_dataset(file_path, dataset_config, repo):\n    dataset = load_dataset(dataset_config, data_files=file_path)\n    \n    dataset.push_to_hub(repo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_path = \"/kaggle/working/output.jsonl\"\ndataset_repo = \"jaymanvirk/synthetic_parallel_corpora\"\ndataset_config = \"json\"\ntmp = load_dataset(dataset_repo\n                     , download_mode = \"force_redownload\")\nlast_index = tmp[\"train\"].num_rows\n\nwith open(output_path, 'w', encoding='utf-8') as json_file:\n    for x in tmp['train']:\n        json_line = json.dumps(x, ensure_ascii=False)\n        json_file.write(json_line + '\\n')\n\ndel tmp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 50\nnum_iters = 200\nthreshold = last_index + 500\nend = last_index+num_iters\nrng = range(last_index, end)\n\nfor i in rng:\n    prompt = f'''\n                Create {batch_size} unique and diverse sentences that cover a wide range of topics, emotions, and styles.\n                '''\n    gen_text = get_gen_text(prompt\n                             , mistral_model\n                             , mistral_tokenizer\n                             , mistral_device)\n\n    translation = [t5_translator(f\"translate English to German: {x}\")[0]['translation_text'] for x in gen_text]\n    \n    with open(output_path, \"a\") as f:\n        for j in range(len(gen_text)):\n            tmp = {'id': (j+last_index), 'translation': {'en': gen_text[j], 'de': translation[j]}}\n            f.write(json.dumps(tmp) + \"\\n\")\n    \n    last_index += len(gen_text)\n    \n    if last_index >= threshold:\n        print(f\"uploading {last_index} records to HF\")\n        threshold += 500\n        push_dataset(output_path, dataset_config, dataset_repo)\n    \n    print(f\"iteration: {i+1}/{end} | completed: {last_index}\")\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}