{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom huggingface_hub import notebook_login","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-21T15:49:00.484621Z","iopub.execute_input":"2023-11-21T15:49:00.485084Z","iopub.status.idle":"2023-11-21T15:49:00.496260Z","shell.execute_reply.started":"2023-11-21T15:49:00.485045Z","shell.execute_reply":"2023-11-21T15:49:00.495349Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dialogue(prompt, model, tokenizer, max_length=100, num_return_sequences=1):\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n\n    # Generate response using the model\n    output = model.generate(\n        input_ids,\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        no_repeat_ngram_size=2,\n        top_k=50,\n        top_p=0.95,\n        temperature=0.7,\n    )\n\n    # Decode the generated response\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text","metadata":{"execution":{"iopub.status.busy":"2023-11-21T15:46:40.279856Z","iopub.execute_input":"2023-11-21T15:46:40.280448Z","iopub.status.idle":"2023-11-21T15:46:40.287852Z","shell.execute_reply.started":"2023-11-21T15:46:40.280410Z","shell.execute_reply":"2023-11-21T15:46:40.286805Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model_name = \"meta-llama/Llama-2-7b\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Generate dialogue\nprompt = \"User: Write a dialogue of 30 messages about public transport in Spain\"\ndialogue = generate_dialogue(prompt, model, tokenizer)\n\nprint(\"Generated Dialogue:\")\nprint(dialogue)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}