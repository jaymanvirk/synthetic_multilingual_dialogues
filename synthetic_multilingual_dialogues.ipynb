{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4295,"sourceType":"modelInstanceVersion","modelInstanceId":3090},{"sourceId":5111,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3899}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport transformers\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-11-24T16:09:38.453896Z","iopub.execute_input":"2023-11-24T16:09:38.454859Z","iopub.status.idle":"2023-11-24T16:09:41.850566Z","shell.execute_reply.started":"2023-11-24T16:09:38.454817Z","shell.execute_reply":"2023-11-24T16:09:41.849575Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_response(prompt, tokenizer, llm_pipeline):\n    sequences = llm_pipeline(\n        prompt,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,\n    )\n    return sequences\n\ndef get_pipeline(model):\n    pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    return pipeline\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-24T16:11:32.121162Z","iopub.execute_input":"2023-11-24T16:11:32.122164Z","iopub.status.idle":"2023-11-24T16:11:32.128451Z","shell.execute_reply.started":"2023-11-24T16:11:32.122131Z","shell.execute_reply":"2023-11-24T16:11:32.127504Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"model = \"/kaggle/input/llama-2/pytorch/7b-hf/1\"\nprompt = \"List top 3 open-source multilingual LLMs\"\ntokenizer = AutoTokenizer.from_pretrained(model)\nllm_pipeline = get_pipeline(model)\nresponse = get_response(prompt, tokenizer, llm_pipeline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}