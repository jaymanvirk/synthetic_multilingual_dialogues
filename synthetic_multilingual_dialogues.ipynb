{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4295,"sourceType":"modelInstanceVersion","modelInstanceId":3090}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport transformers\nimport torch\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-11-25T22:29:19.794903Z","iopub.execute_input":"2023-11-25T22:29:19.795220Z","iopub.status.idle":"2023-11-25T22:29:24.706896Z","shell.execute_reply.started":"2023-11-25T22:29:19.795192Z","shell.execute_reply":"2023-11-25T22:29:24.705823Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def get_sequences(prompt, tokenizer, llm_pipeline):\n    sequences = llm_pipeline(\n        prompt,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,\n    )\n    return sequences\n\ndef get_pipeline(model):\n    pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    return pipeline\n\ndef get_response(gen_text):\n    pattern = r'\\[([^]]+)\\]'\n    matches = re.findall(pattern, gen_text)\n    \n    return matches","metadata":{"execution":{"iopub.status.busy":"2023-11-25T22:29:28.817040Z","iopub.execute_input":"2023-11-25T22:29:28.817544Z","iopub.status.idle":"2023-11-25T22:29:28.825010Z","shell.execute_reply.started":"2023-11-25T22:29:28.817511Z","shell.execute_reply":"2023-11-25T22:29:28.823792Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model_name = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm_pipeline = get_pipeline(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T22:29:34.420811Z","iopub.execute_input":"2023-11-25T22:29:34.421181Z","iopub.status.idle":"2023-11-25T22:32:53.623676Z","shell.execute_reply.started":"2023-11-25T22:29:34.421150Z","shell.execute_reply":"2023-11-25T22:32:53.622753Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ab834e007f049d8ab21cfb50cac164a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"Suggest 10 topics for the dialogue, where each suggestion is wrapped in square brackets.\"\nresponse = get_sequences(prompt, tokenizer, llm_pipeline)\ntopics = response[0][\"generated_text\"].replace(prompt, \"\")\ntopics = get_response(topics)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T22:33:38.413005Z","iopub.execute_input":"2023-11-25T22:33:38.413648Z","iopub.status.idle":"2023-11-25T22:33:49.079533Z","shell.execute_reply.started":"2023-11-25T22:33:38.413616Z","shell.execute_reply":"2023-11-25T22:33:49.078509Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"d = {'question':[], 'answer':[]}\nresponse = f\"Let's discuss this topic: {topics[0]}?\"\nfor i in range(6):\n    if i%2 == 0:\n        key = 'question'\n    else:\n        key = 'answer'\n    prompt = f'''\n        You are in a dialogue with your friend.\n        Your friend says: \"{response}\"\n        Continue the dialogue with your concise {key} in square brackets.\n    '''\n    seq = get_sequences(prompt, tokenizer, llm_pipeline)\n    seq = seq[0][\"generated_text\"].replace(prompt, \"\")\n    resp = get_response(seq)\n    response = resp[0] if len(resp) > 0 else \"Could you repeat?\"\n    d[key].append(response)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T22:34:18.150278Z","iopub.execute_input":"2023-11-25T22:34:18.150956Z","iopub.status.idle":"2023-11-25T22:34:30.636551Z","shell.execute_reply.started":"2023-11-25T22:34:18.150918Z","shell.execute_reply":"2023-11-25T22:34:30.635695Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}