{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate sacrebleu","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (AutoTokenizer, DataCollatorForSeq2Seq,\nAutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer)\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom huggingface_hub import notebook_login\n\nimport numpy as np\nimport evaluate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [PREFIX + example[SRC_LANG] for example in examples[\"translation\"]]\n    targets = [example[TGT_LANG] for example in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs\n                             , text_target = targets\n                             , max_length = MAX_LENGTH\n                             , padding = \"max_length\"\n                             , truncation = True)\n    return model_inputs\n\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n\n    return preds, labels\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_CHECKPOINT = \"t5-base\"\nSRC_LANG = \"en\"\nTGT_LANG = \"de\"\nPREFIX = \"translate English to German: \"\nMAX_LENGTH = 128","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = load_dataset(\"facebook/flores\", \"eng_Latn-deu_Latn\")\nen = t[\"dev\"][\"sentence_eng_Latn\"] + t[\"devtest\"][\"sentence_eng_Latn\"]\nde = t[\"dev\"][\"sentence_deu_Latn\"] + t[\"devtest\"][\"sentence_deu_Latn\"]\ntranslation = [{\"en\": en_sentence, \"de\": de_sentence} for en_sentence, de_sentence in zip(en, de)]\n\ntest_data = Dataset.from_dict({\n                \"id\": list(range(t[\"dev\"].num_rows + t[\"devtest\"].num_rows))\n                ,\"translation\": translation\n            })\n\ndel t, translation, en, de","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rw_train_data = load_dataset(\"opus_books\", lang1=TGT_LANG, lang2=SRC_LANG)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_data = DatasetDict({\n    \"train\": rw_train_data[\"train\"]\n    ,\"test\": test_data\n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_data = split_data.map(preprocess_function, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer\n                                       , model = MODEL_CHECKPOINT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric = evaluate.load(\"sacrebleu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_trainer(output_dir = None, model = None, token_data = None\n                , tokenizer = None, data_collator = None\n                , compute_metrics = None):\n    training_args = Seq2SeqTrainingArguments(\n        output_dir = output_dir,\n        evaluation_strategy = \"epoch\",\n        learning_rate = 5e-5,\n        per_device_train_batch_size = 16,\n        per_device_eval_batch_size = 16,\n        weight_decay = 0.01,\n        save_total_limit = 3,\n        num_train_epochs = 3,\n        predict_with_generate = True,\n        fp16 = True,\n        push_to_hub = True,\n        report_to=\"none\"\n    )\n\n    trainer = Seq2SeqTrainer(\n        model = model,\n        args = training_args,\n        train_dataset = token_data[\"train\"],\n        eval_dataset = token_data[\"test\"],\n        tokenizer = tokenizer,\n        data_collator = data_collator,\n        compute_metrics = compute_metrics,\n    )\n    \n    return trainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = \"t5_base_fine_tuned_opus_books_en_de\"\ntrainer = get_trainer(output_dir = output_dir, model = model, token_data = token_data\n                , tokenizer = tokenizer, data_collator = data_collator\n                , compute_metrics = compute_metrics)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate(max_length=MAX_LENGTH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate(max_length=MAX_LENGTH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sn_train_data = load_dataset(\"jaymanvirk/synthetic_parallel_corpora\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_data = DatasetDict({\n    \"train\": sn_train_data[\"train\"]\n    ,\"test\": test_data\n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_data = split_data.map(preprocess_function, batched=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = \"t5_base_fine_tuned_synthetic_en_de\"\ntrainer = get_trainer(output_dir = output_dir, model = model, token_data = token_data\n                , tokenizer = tokenizer, data_collator = data_collator\n                , compute_metrics = compute_metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate(max_length=MAX_LENGTH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate(max_length=MAX_LENGTH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")","metadata":{},"execution_count":null,"outputs":[]}]}