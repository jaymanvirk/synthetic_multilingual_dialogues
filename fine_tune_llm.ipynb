{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, DataCollatorForSeq2Seq,\nAutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nfrom datasets import load_dataset\nimport evaluate\nimport numpy as np","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n    targets = [example[target_lang] for example in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=MAX_LENGTH, truncation=True)\n    return model_inputs\n\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n\n    return preds, labels\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nl_raw_data = load_dataset(\"opus_books\", lang1=\"en\", lang2=\"nl\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nl_split_data = nl_raw_data[\"train\"].train_test_split(train_size=0.9, seed=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nl_split_data[\"validation\"] = nl_split_data.pop(\"test\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = \"t5-base\"\nsource_lang = \"en\"\ntarget_lang = \"nl\"\nprefix = \"translate English to Dutch: \"\nMAX_LENGTH = 128","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nl_token_data = nl_split_data.map(preprocess_function, batched=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer\n                                       , model = model_checkpoint)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric = evaluate.load(\"sacrebleu\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"t5-base_fine-tuned_opus-books_en-nl\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=3,\n    predict_with_generate=True,\n    fp16=True,\n    push_to_hub=True,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=nl_token_data[\"train\"],\n    eval_dataset=nl_token_data[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate(max_length=MAX_LENGTH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate(max_length=MAX_LENGTH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")","metadata":{},"execution_count":null,"outputs":[]}]}